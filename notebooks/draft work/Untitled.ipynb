{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd20e9a-07ab-472e-9137-99826693c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arjun Joshi. Brainstation Capstone <br>\n",
    "December, 2024<br>\n",
    "Diploma Program: Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff783fe-c325-48d5-91fd-bd72f8840c34",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# <html>\n",
    "# <style>\n",
    "# sup {\n",
    "#     vertical-align: super;\n",
    "#     font-size: smaller;\n",
    "# }\n",
    "# </style>\n",
    "# <p>\n",
    "# <sub><h3><center>Capstone. Case Study: Application of Anthropogenic WSM </center></h3></sub>\n",
    "# <sup></sup><h1> <center>DATA CLEANING, EDA & PRE-PROCESSING</sup>\n",
    "# <sup><h3> .ANTHROPOGENIC PRESSURES. </h3></sup><center></center></h1>\n",
    "# </html>\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47deedc-b40d-4396-852b-c99866c54036",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [markdown]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Arjun Joshi. Brainstation Capstone <br>\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# December, 2024<br>\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Diploma Program: Data Science\u001b[39;00m\n\u001b[0;32m      5\u001b[0m [markdown]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'markdown' is not defined"
     ]
    }
   ],
   "source": [
    "# ##### Run intro scripts\n",
    "\n",
    "# Set working directory to notebook dir\n",
    "import os\n",
    "os.chdir('J:\\\\Brainstation\\\\BS Git\\\\Tampa_Apollo_AnthropogenicIndex\\\\notebooks')\n",
    "%run scripts/imports.py\n",
    "%run scripts/DataDictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673a944-9ce1-49f2-a34d-8c367f4a8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30477df8-04da-423b-81d1-afa4399ad283",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# ### **Introduction : Context and Notebook Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80f0ab-7631-4f12-8ba7-72f58dce6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# As stated in the ReadMe, 3 supplementary notebooks cover cleaning, EDA, requisite baseline modelling for each individual environment/ecosystem/overarching framework - which are ultimately used to measure the performance of the Anthropogenic Pressure Index (API)\n",
    "# \n",
    "# Figure 1 below shows the geography, scale and basic information about the Tampa Region. The marked area in white in the top left image shows the town where the marine construction in this case study has been proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eaee24-8380-4742-8c07-404765d3d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# ![MAPS.png](attachment:ba2ee358-09be-4a38-b81b-24098c4c82e6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bc5ef-90b8-4dfe-b215-a5b776126d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# <sup>Figure1.<br>\n",
    "# > (A) Top L. marked sections of Tampa Bay <br>\n",
    "# > (B) Top R. Sampling Sites Near Proposed Construction site. prominent filled circles are active, all others are not inactive <br>\n",
    "# > (C) Bottom L. 2020 Population Density of Tampa Bay Region <br>\n",
    "# > (D) Bottom R. The massive scale of the watershed, the marked area is the HB watershed and the one directly underneath is MTB. <br>Marking in the >     center of the the image is the proposed study site </sup>\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d38612-cbc7-4f0d-8e90-fd8927a2f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# >>> Regional Watershed Area Descriptions.<br>\n",
    "# >>> **HB Watershed**: Area (Sq Miles) : 1,281.83 <br>\n",
    "# >>> **MTB watershed**: Area (Sq Miles) : 410.31 <br>\n",
    "# \n",
    "# When considering the nutrient loading, nitrogen in particular, it's important to understand the scale of the watershed - all of which flows into the bay. The H.C. is massive, covering nearly 1300 sq miles. That area includes large amounts of agricultral land, new residential and commercial construction.\n",
    "# \n",
    "# Notebook Context: This notebook processes what is considered to be data describing Anthropogenic Pressures on the coastal ecosystem.\n",
    "# Notebook Purpose: Clean, Pre-process Anth.Gen. data, including EDA and baseline modelling, in order to establish crucial relationships between features.<br> This notebook requires an understanding of Nitrogen compounds in the ecosystem, if they're organic or not and their patterns over time. Unlike the biology specific data, Anth. Gen. data obtained has a sampling structure and consistency to attempt a Time-Series Analysis.\n",
    "# <br>\n",
    "# <br>\n",
    "# *I encourage anyone reviewing the code to utilize the Table of Contents for easy navigation.*<br>\n",
    "# Each subcategory has it's own section for cleaning, EDA, processing and baselines modelling (if applicable)\n",
    "# There are 3 major areas of focus covered, the notebook is also organized accordingly:<BR>\n",
    "# 1. Section: Nitrate Concentrations, Water Quality (ADF, TSA)\n",
    "# 2. Section: Property Development Metrics\n",
    "# 3. Section: Population Growth Metrics (Linear Regression)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07635dfc-721e-4d1a-866f-cfe167ad9710",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# ##### Post-Intro Note : 2 Features Omitted\n",
    " [markdown]\n",
    "# *Two additional possible sources of anth. pressures were in the original design of the study: marine recreational traffic and geospatial analysis. These were omitted because it was not feasible to get records of marine traffic without paying a respectable amount for GIS-hosted GPS data for vessels. Data obtained did not include GPS during a trip, only start, end and if any stops were made or ports called upon. Geospatial data had to be omitted due to loss of data due to hardware issues. This will be pursued in subsequent expansion on this study.*\n",
    "# \n",
    "# *Please refer to the ReadMe for a more detailed look into the complexities and impacts of Anth. Gen. on coastal marine ecosystems.*\n",
    " [markdown]\n",
    "# #### Fx, Definitions, Read Data\n",
    " [markdown]\n",
    "# #### Functions (if applicable)\n",
    " [markdown]\n",
    "# #### Read CSV into Pandas DF\n",
    " [markdown]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac880a1-349c-4116-9caf-78f4d19bcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Nutrient Loading Data\n",
    "os.chdir('J:/Brainstation/BS Git/data/human Imact')\n",
    "# There are 4 sections of Tampa Bay.\n",
    "# This study covers 2 but can expanded to all. Refer to map in introduction section\n",
    "# Import Middle Tampa Bay (MTB) Water Quality (WQ) data. 1990-2024\n",
    "mtbwq=pd.read_csv('MTB_WQ.csv',low_memory=False)\n",
    "# Import Hillsborough Bay (HB) W.Q. data. 1990-2024\n",
    "hbwq=pd.read_csv('HB_WTQ.csv',low_memory=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5213d3-d957-4272-999e-af9df7d00685",
   "metadata": {},
   "outputs": [],
   "source": [
    "[markdown]\n",
    "##### Property Development\n",
    "# New data directory\n",
    "os.chdir('J:/Brainstation/BS Git/Property Records')\n",
    "proprx=pd.read_csv('PRIMARY_PROPRX.csv',low_memory='False')\n",
    "# Subset dfs\n",
    "# Original Sectioned Property Records\n",
    "AB_1=pd.read_csv('AB_1.csv')\n",
    "AB_2=pd.read_csv('AB_BLVD_SPIT_1.csv')\n",
    "AB_3=pd.read_csv('AB_Central_1.csv')\n",
    "AB_4=pd.read_csv('AB_INTSXN_1.csv')\n",
    "AB_5=pd.read_csv('AB_SOUTH_1.csv')\n",
    "AB_6=pd.read_csv('AB_SOUTH_2.csv')\n",
    "AB_7=pd.read_csv('AB_SPIT_1.csv')\n",
    "GIB_8=pd.read_csv('GIB_MAIN_1.csv')\n",
    "RIV_1=pd.read_csv('RIV_W_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69f897-f6a6-4105-a5f9-8045435cc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "[markdown]\n",
    "# ##### Population Growth\n",
    "os.chdir('J:\\\\Brainstation\\\\BS Git\\\\data\\\\human Imact\\\\MarineTraffic')\n",
    "dfpop_staged=pd.read_csv('StagedPopulations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f93c4a-625d-43e8-9d55-08c6d58ccd48",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# ## Feature Space 1: Nitrate Loading; Water Quality Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c39d5-4822-4851-87e5-eaabb5cc1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# Tables loaded in at start of notebook.<br>Tables:<BR>\n",
    "# > _(hbwq)_   Hillsborough Bay (HB)<br>\n",
    "# > _(mtbwq)_ Middle Tampa Bay (MTB)<br>\n",
    "# These 2 data sets are closest in proximmity to the study site, the focus of the study. <br>\n",
    "# This dataset can always be expanded at a later date.\n",
    "# \n",
    "# Objective: <BR>Establish clear pattern between 3 forms of Nitrogen compounds in the bay:<br>\n",
    "# Nitrates and Nitrates (NO3,NO2-).   Ammonia/Ammonium (NH3).       Organic Nitrogen.\n",
    "# \n",
    "# Key Info:<BR>\n",
    "# Nitrates/Nitrites (NO3, No2-), and ammonia(NH3/NH4+) are inorganic forms of nitrogen and either represent the slow breakdown of organic material or input from external sources. Part 1 must demonstrate that NH3 and N03 compound(s) vary, with significance, from behaviors and patterns observed in Organic Nitrogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5db5bd-03b9-4947-8918-2694cf5f45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtbwq.head(5)\n",
    "# Df read-in check\n",
    "hbwq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b317af6-1808-4959-b738-7964d09893f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df read-in check\n",
    "mtbwq.head(5)\n",
    "#null check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3b574-da90-4d20-9318-576ccd6a6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtbwq.isna().sum())\n",
    "hbwq.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbc0e8-0a1c-4394-b945-b50ccb176548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column names\n",
    "mtbwq.Actual_Latitude\n",
    "mtbwq.columns[[6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa72f1-3e91-422b-8b23-a22ce9355394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing some columns between these 2 datasets\n",
    "hbwq.rename(columns={'Latitude_DD':'Latitude','Longitude_DD':'Longitude'},inplace=True)\n",
    "hbwq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa0a1e-a372-4ad2-9a36-11207169bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing some columns between these 2 datasets\n",
    "mtbwq.rename(columns={'Actual_Latitude':'Latitude','Actual_Longitude':'Longitude'},inplace=True)\n",
    "mtbwq.head(5)\n",
    "hbwq.Characteristic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b0d28-bed3-4621-8660-13e0dbdd8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# ** This study uses Organic and Inorganic forms of nitrogen and phosphorous as a proxy for anthropogenic introduction of harmful chemicals into the bay. This study examines these 2 features in 2 main ways:<br>\n",
    "# - As an aggregate statistic that describes the functioning of the bay at large\n",
    "# - The  ratio of organic to inorganic  compounds that describe that extent of anthropogenic and ultimately harmful versions of these compounds introduced.\n",
    "# looking specifically for the forms of Nitrogen.\n",
    "# Nitrogen comes in Nitrogen Organic, Nitrogen, Nitrates, Nitrites, Ammonia, Ammonium\n",
    "# HBWQ - Hillsborough Bay Water Quality\n",
    "# MTBWQ - Middle Tampa Bay Water Quality\n",
    "index_1=hbwq.Characteristic.str.contains('Nit,nit').index==True\n",
    "len(index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bf2b3-5619-443a-bd39-d90937a030d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate Nit. in HB\n",
    "Nit=hbwq.Characteristic.str.contains('Nitrogen')\n",
    "Nit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9263841-2523-4400-a3f7-f953a692c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phosphate isolation. Omitted.\n",
    "# Remnant from initial EDA \n",
    "pho_hb=hbwq.Characteristic.str.contains('Phosph')\n",
    " [markdown]\n",
    "# ### Data Wrangling, EDA / Pre-Processing\n",
    " [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db5004-fcb6-47de-9bc5-f9fb78044d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### WQI Sample Frequency, Consistency Check\n",
    "# Begin EDA, aggregations for patterns\n",
    "hbwq.SampleDate.str.slice(-4).value_counts().reset_index().sort_values(by='SampleDate')\n",
    "hbwq.SampleDate=pd.to_datetime(hbwq.SampleDate)\n",
    "mtbwq.SampleDate=pd.to_datetime(mtbwq.SampleDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f3abc-31dd-45e9-8ff3-dff4cd489262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "hbwq.head(3)\n",
    "mtbwq.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164de5cf-53ce-4d19-979f-684096ac23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at sample counts over time.\n",
    "print(hbwq[hbwq.SampleDate.dt.year==1990].SampleDate.dt.month.value_counts().reset_index().sort_values(by='SampleDate'))\n",
    "hbwq[hbwq.SampleDate.dt.year==1990].SampleDate.dt.month.value_counts().reset_index().sort_values(by='SampleDate').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e40d0-a163-43ff-8554-6e1703533aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for distribution of sample counts by month\n",
    "print(mtbwq[mtbwq.SampleDate.dt.year==1990].SampleDate.dt.month.value_counts().reset_index().sort_values(by='SampleDate'))\n",
    "mtbwq[mtbwq.SampleDate.dt.year==1990].SampleDate.dt.month.value_counts().reset_index().sort_values(by='SampleDate').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09a5a7-c9f2-48b9-9772-150dc08a71df",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84468280-2b3b-4a51-8aab-4f4f7fb62a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "[markdown]\n",
    "# #### _Preliminary EDA and processing of subset, 1990, used in initial EDA and construction of baseline models. CODE OUTDATED_\n",
    "# MTB. HB. Isolate for 1990\n",
    "mtbwq1990=mtbwq[mtbwq.SampleDate.dt.year==1990]\n",
    "hbwq1990=hbwq[hbwq.SampleDate.dt.year==1990]\n",
    "# Exploring at data for 1 month\n",
    "mtbwq1990[mtbwq1990.SampleDate.dt.month==1].sort_values(by='SampleDate')\n",
    "# Format dt values\n",
    "mtbwq.SampleDate=pd.to_datetime(mtbwq.SampleDate)\n",
    "hbwq.SampleDate=pd.to_datetime(hbwq.SampleDate)\n",
    "# Look at sampling\n",
    "hbwq1990_samplefreq=hbwq1990.SampleDate.value_counts().reset_index().sort_values(by='SampleDate').reset_index(drop=True)\n",
    "hbwq1990_samplefreq.SampleDate.hist()\n",
    "# Looking for missing weekly samples\n",
    "import matplotlib.dates as mdates\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.histplot()\n",
    "ax = sns.histplot(x=hbwq1990_samplefreq.SampleDate,bins=365)\n",
    "ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45)\n",
    "#plt.xticks(range('1990-01-01','1990-12-31', 365))\n",
    "plt.show()\n",
    "#sanity\n",
    "hbwq1990.head(2)\n",
    "# Isolate Nitrates, complete examination of subset of data\n",
    "nitrogen_1990=hbwq1990[hbwq1990.Characteristic.str.contains('Nitr','nitr')==True]\n",
    "nitrogen_1990.head(5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c87e6-8da3-40e2-97f0-da40d33f31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[markdown]\n",
    "# ##### EDA: Isolation of Nitrogen Compounds, 1990 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee653273-c830-4399-b064-e8bbb249cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# The dataset has multiple entries for each sample event. The same sample is analyzed and described in multiple ways. Consequently, data needs to be isolated to get a consistent single data set.\n",
    "# Create Phosphate df\n",
    "phosphate_hb=hbwq[hbwq.Characteristic.str.contains('Phos','phos')==True]\n",
    "phosphate_hb.SampleDate=pd.to_datetime(phosphate_hb.SampleDate)\n",
    "phosphate_hb.head(2)\n",
    "#Isolate Phosphate\n",
    "phosphate_1990=hbwq1990[hbwq1990.Characteristic.str.contains('Phos','phos')==True]\n",
    "phosphate_1990.head(5)\n",
    "# Checking Units\n",
    "pd.concat([nitrogen_1990.Characteristic.value_counts(),nitrogen_1990.ResultUnit.value_counts()],axis=1)\n",
    "# Isolate Nit\n",
    "nitrate1990=nitrogen_1990[nitrogen_1990.Characteristic.str.contains('Nitrogen, Nitrite')]\n",
    "print(nitrate1990.shape)\n",
    "nitrate1990.reset_index(drop=True,inplace=True)\n",
    "nitrate1990.head(3)\n",
    "# Test nitrate isolation\n",
    "nitrate1990[nitrate1990.ResultUnit.isna()]\n",
    "nitrate1990.ResultUnit.fillna('ug/l',inplace=True)\n",
    "nitrate1990[nitrate1990.ResultUnit.isna()]\n",
    "# Standardize units\n",
    "for i in nitrate1990.index:\n",
    "    if nitrate1990.loc[i,'ResultUnit']=='ug/l':\n",
    "        nitrate1990.loc[i,'ResultValue']=nitrate1990.loc[i,'ResultValue']/1000\n",
    "        nitrate1990.loc[i,'ResultUnit']='mg/l'\n",
    "        #ammonia1990.ResultValue=ammonia1990.ResultValue/1000\n",
    "# Create subset with features of interest\n",
    "nitrate_subset=nitrate1990[['DataSourceName','StationID','SampleDate','SampleTime','ResultValue','ResultUnit','Latitude','Longitude']]\n",
    "# date format\n",
    "nitrate_subset['month']=nitrate_subset.SampleDate.dt.month\n",
    "nitrate_subset\n",
    "# look at nitrate patterns over 1 year period\n",
    "nitrate_subset_plot=nitrate_subset.groupby('month').ResultValue.agg(['mean','std','min','max'])\n",
    "nitrate_subset_plot.head(12)\n",
    "# Isolate ammmonia\n",
    "ammonia1990=nitrogen_1990[nitrogen_1990.Characteristic.str.contains('ammon')].sort_values(by='SampleDate').reset_index(drop=True)\n",
    "print(ammonia1990.shape)\n",
    "ammonia1990.head()\n",
    "# Check for units for standardization\n",
    "ammonia1990.ResultUnit.value_counts()\n",
    "# Will need to standardize units\n",
    "ammonia1990[ammonia1990.ResultUnit=='ug/l'].ResultValue\n",
    "# Std. ammonia units\n",
    "for i in ammonia1990.index:\n",
    "    if ammonia1990.loc[i,'ResultUnit']=='ug/l':\n",
    "        ammonia1990.loc[i,'ResultValue']=ammonia1990.loc[i,'ResultValue']/1000\n",
    "        ammonia1990.loc[i,'ResultUnit']='mg/l'\n",
    "        #ammonia1990.ResultValue=ammonia1990.ResultValue/1000\n",
    "ammonia1990.ResultUnit.value_counts()\n",
    "ammonia_subset=ammonia1990[['DataSourceName','StationID','SampleDate','SampleTime','ResultValue','ResultUnit','Latitude','Longitude']]\n",
    "#Drop amm. dupes\n",
    "print(ammonia_subset.duplicated().value_counts())\n",
    "ammonia_subset=ammonia_subset.drop_duplicates()\n",
    "print(ammonia_subset.duplicated().value_counts())\n",
    "ammonia_subset.head(5)\n",
    "# any null values would be in ug/l format\n",
    "ammonia_subset.ResultUnit.fillna('ug/l',inplace=True)\n",
    "# Standardize \n",
    "for i in ammonia_subset.index:\n",
    "    if ammonia_subset.loc[i,'ResultUnit']=='ug/l':\n",
    "        ammonia_subset.loc[i,'ResultValue']=ammonia1990.loc[i,'ResultValue']/1000\n",
    "        ammonia_subset.loc[i,'ResultUnit']='mg/l'\n",
    "        #ammonia1990.ResultValue=ammonia1990.ResultValue/1000\n",
    "# date format\n",
    "ammonia_subset['month']=ammonia_subset.SampleDate.dt.month\n",
    "# aggregate amm. to monthly for 1 year, look at patterns\n",
    "ammonia_subset_plot=ammonia_subset.groupby('month').ResultValue.agg(['mean','std','min','max'])\n",
    "ammonia_subset_plot.head(12)\n",
    "pwd\n",
    "# look at monthly aggreate patterns for 1 year\n",
    "sns.lineplot(x='month',y='mean',data=ammonia_subset_plot,errorbar=('ci',95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbb568-f7fd-4671-8ccf-85d77f10dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "[markdown]\n",
    "# Kjeldahl units are a measurement of nitrogen that typically represents concentrations of organic Nitrogen. It is an engineered measurement, where a water sample is treated with a catalytic agent to convert nitrogen into ammonia, similar to the decomposition process.\n",
    "# Isolation kjedahl measurements\n",
    "kjedahl=nitrogen_1990[nitrogen_1990.Characteristic.str.contains('Kjeld')]\n",
    "print(kjedahl.shape)\n",
    "kjedahl.head(5)\n",
    "print(kjedahl.ResultUnit.isna().value_counts())\n",
    "kjedahl.ResultUnit.value_counts()\n",
    "# Look at units\n",
    "kjedahl[kjedahl.ResultUnit=='ug/l'].head()\n",
    "# std. kjel units\n",
    "for i in kjedahl.index:\n",
    "    if kjedahl.loc[i,'ResultUnit']=='ug/l':\n",
    "        kjedahl.loc[i,'ResultValue']=kjedahl.loc[i,'ResultValue']/1000\n",
    "        kjedahl.loc[i,'ResultUnit']='mg/l'\n",
    "        #ammonia1990.ResultValue=ammonia1990.ResultValue/1000\n",
    "kjedahl.reset_index(drop=True,inplace=True)\n",
    "kjedahl.sort_values(by='SampleDate').reset_index(drop=True,inplace=True)\n",
    "#date format\n",
    "kjedahl['month']=kjedahl.SampleDate.dt.month\n",
    "# isolate kjel features of interest\n",
    "kjedahl_subset=kjedahl[['DataSourceName','StationID','SampleDate','SampleTime','ResultValue','ResultUnit','Latitude','Longitude','month']]\n",
    "# sanity check\n",
    "kjedahl_subset.head(5)\n",
    "# search for inconsistent values, outlier values\n",
    "kjedahl_subset[kjedahl_subset.ResultValue>2]\n",
    "# Kjeldahl measurements are typically higher than that of NO3 or NH4 - mg/l were initially chosen here.\n",
    "# NO3 and NH4 were later standardized to mg/l when processing the full dataset\n",
    "# std. units to mg/l\n",
    "for i in kjedahl.index:\n",
    "    if kjedahl_subset.loc[i,'ResultValue']>2:\n",
    "            kjedahl_subset.loc[i,'ResultValue']=kjedahl_subset.loc[i,'ResultValue']/1000\n",
    "            kjedahl_subset.loc[i,'ResultUnit']='mg/l'\n",
    "        #ammonia1990.ResultValue=ammonia1990.ResultValue/1000\n",
    "# monthly aggregations to look at subset annual pattern\n",
    "kjedahl_subset_plot=kjedahl_subset.groupby('month').ResultValue.agg(['mean','std','max','min'])\n",
    "kjedahl_subset_plot.head(12)\n",
    "# Patterns\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "sns.lineplot(x='month',y='mean',data=kjedahl_subset_plot,errorbar=('ci',.95),color='green',ax=ax)\n",
    "ax1=ax.twinx()\n",
    "sns.lineplot(x='month',y='mean',data=ammonia_subset_plot,errorbar=('ci',.95),color='red',ax=ax1)\n",
    "sns.lineplot(x='month',y='mean',data=nitrate_subset_plot,errorbar=('ci',.95),color='blue',ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bb91d-9443-4ff3-8502-86bd1d2361cd",
   "metadata": {},
   "outputs": [],
   "source": [
    " [markdown]\n",
    "# #### Processing Full Nitrate Dataset\n",
    "hbconcat=hbwq[['DataSourceName','StationID','Latitude','Longitude','SampleDate','SampleTime','Characteristic','ResultUnit','ResultValue','WaterbodyName']]\n",
    "hbconcat.head(3)\n",
    "# Daily sample times are not relevant. setting to constant to avoid bad aggregations\n",
    "mtbwq['SampleTime']='0:00:00'\n",
    "mtbwq.head(1)\n",
    "# isolate features\n",
    "mtbconcat=mtbwq[['DataSource','StationID','Latitude','Longitude','SampleDate','SampleTime','Characteristic','Result_Unit','Result_Value','WaterBodyName']]\n",
    "mtbconcat.head(3)\n",
    "# null check\n",
    "mtbconcat.DataSource.isna().value_counts()\n",
    "# rename units to match HB\n",
    "mtbconcat.rename(columns={'WaterBodyName':'WaterbodyName','DataSource':'DataSourceName','Result_Unit':'ResultUnit','Result_Value':'ResultValue'},inplace=True)\n",
    "mtbconcat.head(1)\n",
    "# Union HB and MTB into 1 df\n",
    "combwq=pd.concat([hbconcat,mtbconcat])\n",
    "combwq.head(5)\n",
    "# check nitrogen compounds isolated\n",
    "combwq[combwq.Characteristic.str.contains('Nit')==True].Characteristic.value_counts()\n",
    "combwq.shape\n",
    "# Export data for processing outside of notebook\n",
    "combwq.to_csv('NIT_FINAL.csv')\n",
    "### There was one sampling station / service that was producing highly irregular nitrate values\n",
    "### I suspect that they were correct - but there were so few measurements that, as significant outliers, it was better\n",
    "### to remove them from the dataset. They were orders higher.\n",
    "### Samples for this station from all datasets needs to be removed\n",
    "combwq.drop(combwq[combwq.StationID=='560'].index,inplace=True)\n",
    "#Checking to ensure iso of nitrates\n",
    "combwq[combwq.Characteristic.str.lower().str.contains('nitra','nitri')==True]\n",
    "combwq[combwq.Characteristic.str.lower().str.contains('nitra','nitri')==True].shape\n",
    "# Isolate each compound of interest\n",
    "nitrates=combwq[combwq.Characteristic.str.lower().str.contains('nitra','nitri')==True]\n",
    "kjeldahl=combwq[combwq.Characteristic.str.lower().str.contains('kjel')==True]\n",
    "ammonia=combwq[combwq.Characteristic.str.lower().str.contains('ammo')==True]\n",
    "print(nitrates.shape)\n",
    "print(kjeldahl.shape)\n",
    "print(ammonia.shape)\n",
    "# Select only mg/l units\n",
    "nitrates_mg=nitrates[nitrates.ResultUnit=='mg/l']\n",
    "kjeldahl_mg=kjeldahl[kjeldahl.ResultUnit=='mg/l']\n",
    "ammonia_mg=ammonia[ammonia.ResultUnit=='mg/l']\n",
    "# sample size reduced. Many rows were duplicates using the other measurement unit.\n",
    "print(nitrates_mg.shape)\n",
    "print(kjeldahl_mg.shape)\n",
    "print(ammonia_mg.shape)\n",
    "# drop null values. Not imputing anything yet\n",
    "nitrates_mg.dropna(inplace=True)\n",
    "kjeldahl_mg.dropna(inplace=True)\n",
    "ammonia_mg.dropna(inplace=True)\n",
    "print(nitrates_mg.shape)\n",
    "print(kjeldahl_mg.shape)\n",
    "print(ammonia_mg.shape)\n",
    "nitrates_mg.ResultValue.isna().value_counts()\n",
    "ammonia_mg.ResultValue.isna().value_counts()\n",
    "kjeldahl_mg.ResultValue.isna().value_counts()\n",
    "# Checking for outliers\n",
    "ammoniasub[ammoniasub.ResultValue>2]\n",
    "# distribution checks. log scale\n",
    "plt.hist(nitrates_mg.ResultValue)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('sample count')\n",
    "plt.xlabel('Concentration Value')\n",
    "plt.title('outlier check:nitrates mg df histogram')\n",
    "plt.show()\n",
    "# distribution checks. log scale\n",
    "plt.hist(ammonia_mg.ResultValue)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('sample count')\n",
    "plt.xlabel('Concentration Value')\n",
    "plt.title('outlier check:ammonia mg df histogram')\n",
    "plt.show()\n",
    "# distribution checks. log scale\n",
    "plt.hist(kjeldahl_mg.ResultValue)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('sample count')\n",
    "plt.xlabel('Concentration Value')\n",
    "plt.title('outlier check:kjeldahl mg df histogram')\n",
    "plt.show()\n",
    "## This value will get removed when the dataset time period is reduced\n",
    "kjeldahl_mg[kjeldahl_mg.ResultValue>10]\n",
    "nitrates_mg_temp=nitrates_mg.reset_index()\n",
    "#nitrates_mg[nitrates_mg.ResultValue>3].reset_index()\n",
    "nit_iloc=nitrates_mg_temp[nitrates_mg_temp.ResultValue>2.5].index\n",
    "for i in nit_iloc:\n",
    "    nitrates_mg.iloc[i,-2]=nitrates_mg.iloc[i,-2]/1000\n",
    "# standardize units\n",
    "kjeldahl_mg['month']=kjeldahl_mg.SampleDate.dt.month\n",
    "ammonia_mg['month']=ammonia_mg.SampleDate.dt.month\n",
    "nitrates_mg['month']=nitrates_mg.SampleDate.dt.month\n",
    "# Isolate features of interest\n",
    "kjeldahlsub=kjeldahl_mg[['DataSourceName','StationID','SampleDate','SampleTime','ResultValue','ResultUnit','Latitude','Longitude','month']]\n",
    "ammoniasub=ammonia_mg[['DataSourceName','StationID','SampleDate','SampleTime','ResultValue','ResultUnit','Latitude','Longitude','month']]\n",
    "nitratesub=nitrates_mg[['DataSourceName','StationID','SampleDate','SampleTime','ResultValue','ResultUnit','Latitude','Longitude','month']]\n",
    "# Using a temp table to work around coding errors\n",
    "# Looking at outlier data. Assess whether unit conv was issue\n",
    "index_amm_temp=ammoniasub[ammoniasub.ResultValue>3].index\n",
    "ammoniasub.loc[index_amm_temp].ResultValue=ammoniasub.loc[index_amm_temp].ResultValue/1000\n",
    "# station suspected culprit for outlier values\n",
    "kjeldahl_mg[kjeldahl_mg.StationID=='560']\n",
    "# create year col\n",
    "kjeldahlsub['year']=kjeldahlsub.SampleDate.dt.year\n",
    "ammoniasub['year']=ammoniasub.SampleDate.dt.year\n",
    "nitratesub['year']=nitratesub.SampleDate.dt.year\n",
    "# creating a col that sets all months to a starting day of 1\n",
    "kjeldahlsub['day']=1\n",
    "ammoniasub['day']=1\n",
    "nitratesub['day']=1\n",
    "# format dt\n",
    "kjeldahlsub['DateMonth']=pd.to_datetime(kjeldahlsub.astype(str).month+'-'+kjeldahlsub.astype(str).day+'-'+kjeldahlsub.astype(str).year)\n",
    "ammoniasub['DateMonth']=pd.to_datetime(ammoniasub.astype(str).month+'-'+ammoniasub.astype(str).day+'-'+ammoniasub.astype(str).year)\n",
    "nitratesub['DateMonth']=pd.to_datetime(nitratesub.astype(str).month+'-'+nitratesub.astype(str).day+'-'+nitratesub.astype(str).year)\n",
    "# isolate dataset to years of interest in the study\n",
    "kjeldahlsub=kjeldahlsub[kjeldahlsub.year>1989]\n",
    "nitratesub=nitratesub[nitratesub.year>1989]\n",
    "ammoniasub=ammoniasub[ammoniasub.year>1989]\n",
    "# create better date column\n",
    "nitratesub.SampleDate=pd.to_datetime(nitratesub.SampleDate.astype(str).str.slice(0,10))\n",
    "kjeldahlsub.SampleDate=pd.to_datetime(kjeldahlsub.SampleDate.astype(str).str.slice(0,10))\n",
    "ammoniasub.SampleDate=pd.to_datetime(ammoniasub.SampleDate.astype(str).str.slice(0,10))\n",
    "#outlier check\n",
    "kjeldahlsub[kjeldahlsub.ResultValue>4]\n",
    " [markdown]\n",
    "#  \n",
    " [markdown]\n",
    "# **Only Kjeldahl in the filtered dataset follow a normal distribution, whereas Nitrates and Ammonia are heavily right-skew**\n",
    "# <br>\n",
    "### Only Kjeldahl has a normal distribution\n",
    "plt.hist(kjeldahlsub.ResultValue,bins=50)\n",
    "#\"plt.yscale('log')\n",
    "plt.title('distribution of kjeldahl values at log scale')\n",
    "plt.text(1.5, 100, f'Num of kjel val above 2mg/L= {kjeldahlsub[kjeldahlsub.ResultValue>4].ResultValue.count()}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()\n",
    "plt.hist(nitratesub.ResultValue,bins=50)\n",
    "plt.yscale('log')\n",
    "plt.title('distribution of nitrate values at log scale')\n",
    "plt.text(1.5, 100, f'Num of NO3 val above 2mg/L= {nitratesub[nitratesub.ResultValue>2].ResultValue.count()}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()\n",
    "plt.hist(ammoniasub.ResultValue,bins=50)\n",
    "plt.yscale('log')\n",
    "plt.title('distribution of ammonia values at log scale')\n",
    "plt.text(1.5, 100, f'Num of NH3 val above 2mg/L= {ammoniasub[ammoniasub.ResultValue>2].ResultValue.count()}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()\n",
    "# Setting checkpoint\n",
    "ksub=kjeldahlsub.copy()\n",
    "asub=ammoniasub.copy()\n",
    "nsub=nitratesub.copy()\n",
    "# Adding some columns to standardize dates and make next steps easier\n",
    "ksub['week']=ksub.SampleDate.dt.isocalendar().week\n",
    "asub['week']=asub.SampleDate.dt.isocalendar().week\n",
    "nsub['week']=nsub.SampleDate.dt.isocalendar().week\n",
    "ksub['adj_date']=pd.to_datetime(ksub['year'].astype(str) + '-W' + ksub['week'].astype(str) + '-1', format='%Y-W%W-%w')\n",
    "asub['adj_date']=pd.to_datetime(asub['year'].astype(str) + '-W' + asub['week'].astype(str) + '-1', format='%Y-W%W-%w')\n",
    "nsub['adj_date']=pd.to_datetime(nsub['year'].astype(str) + '-W' + nsub['week'].astype(str) + '-1', format='%Y-W%W-%w')\n",
    "ksub.drop(columns='DateMonth',inplace=True)\n",
    "asub.drop(columns='DateMonth',inplace=True)\n",
    "nsub.drop(columns='DateMonth',inplace=True)\n",
    "ksub.sort_values(by='SampleDate',inplace=True)\n",
    "asub.sort_values(by='SampleDate',inplace=True)\n",
    "nsub.sort_values(by='SampleDate',inplace=True)\n",
    "# There are missing weeks that need to be imputed. This should be weekly data from 1990-2024\n",
    "# Creating a df with all weeks, including missing values. Will outer join all nitrate groups, remove duplicates, impute or interpolate missing values\n",
    "data = []\n",
    "for year in range(1990, 2025):  # 24 years from 1990 to 2024\n",
    "    for week in range(1, 53):  # Weeks 1 to 52\n",
    "        data.append([year, week])\n",
    "weekly=pd.DataFrame(data=data,columns=['year','week'])\n",
    "weekly['adj_date']=pd.to_datetime(weekly['year'].astype(str) + '-W' + weekly['week'].astype(str) + '-1', format='%Y-W%W-%w')\n",
    "print(len(data))\n",
    "print(len(weekly))\n",
    "print(weekly.min())\n",
    "weekly.max()\n",
    "# Make sure the datasets are as clean as possible (missing values dealt with later)\n",
    "ksub.drop_duplicates(inplace=True)\n",
    "asub.drop_duplicates(inplace=True)\n",
    "nsub.drop_duplicates(inplace=True)\n",
    "print(ksub.shape)\n",
    "print(asub.shape)\n",
    "print(nsub.shape)\n",
    "# outer join kjeldahl df, no loss of data\n",
    "test=weekly.merge(ksub,left_on=weekly.adj_date,right_on=ksub.adj_date,how='outer',suffixes=('_dt','_K'))\n",
    "# adj_date_dt remains complete, dropping key_0\n",
    "test.info()\n",
    "test.drop(columns='key_0',inplace=True)\n",
    "test.head(2)\n",
    "# outer join ammonia results, no loss of data\n",
    "# all units are standardized to mg/l\n",
    "# need to keep key_0 as it's the only complete date column.\n",
    "test2=test.merge(asub['ResultValue'],left_on=[test.adj_date_dt,test.DataSourceName,test.StationID],right_on=[asub.adj_date,asub.DataSourceName,asub.StationID],how='outer',suffixes=('_dt','_A')).drop_duplicates()\n",
    "test2.rename(columns={'key_0':'adj_date'},inplace=True)\n",
    "test2.drop(columns=['key_1','key_2'],inplace=True)\n",
    "# outer join nitrate results, no loss of data\n",
    "# all units are standardized to mg/l\n",
    "# last merge\n",
    "test3=test2.merge(nsub['ResultValue'],left_on=[test2.adj_date,test2.DataSourceName,test2.StationID],right_on=[nsub.adj_date,nsub.DataSourceName,nsub.StationID],how='outer',suffixes=('_dt','_N')).drop_duplicates()\n",
    "test3.info()\n",
    "# Isolate columns of interest\n",
    "nmerge_stage=test3[test3.columns[[0,7,13,14,11,20,21]]]\n",
    "nmerge_stage.head(1)\n",
    "# Rename columns\n",
    "nmerge_stage.columns=['date_adj','DataSourcveName','Lat','Long','ResultValue_K','ResultValue_A','ResultValue_N']\n",
    "print(nmerge_stage.shape)\n",
    "nmerge_stage.head(2)\n",
    "# Establish independent copy of merged base table\n",
    "nmerge_base=nmerge_stage.copy()\n",
    "nmerge_base[nmerge_base.ResultValue_N>2]\n",
    "# Aggregate at weekly date intervals\n",
    "nmerge_stage2=nmerge_stage.groupby('date_adj').agg({\n",
    "    'ResultValue_K':'mean',\n",
    "    'ResultValue_A':'mean',\n",
    "    'ResultValue_N':'mean'\n",
    "})\n",
    "# Extra weeks beyond the end of the dataset (8/26/24)\n",
    "# Cut out extra weeks\n",
    "# Check for nulls\n",
    "nmerge_stage3=nmerge_stage2.iloc[:-17,:]\n",
    "nmerge_stage3.isna().sum()\n",
    " [markdown]\n",
    "# There are fewer missing values than expected! That's good, interpolation shouldn't be a problem. <br>\n",
    "# Seasonality shouldn't be expected for periods less than a month. <br>\n",
    "# Interpolation with 2 period in both directions should give best approximation of values.<br>\n",
    "nmerge=nmerge_stage3.interpolate(method='linear', limit_direction='both', limit=2)\n",
    "print(f'{nmerge.info()}\\n\\n\\nSHAPE:\\n{nmerge.shape}\\n\\n\\n\\nDESCRIBE:\\n{nmerge.describe()}')\n",
    "nmerge.head(2)\n",
    "print(f'nmerge shape:\\n {nmerge.shape}\\n')\n",
    "print(f'######\\n\\nMerge1 duplicates: \\n{nmerge.duplicated().value_counts}\\n\\n######\\nResultAction:')\n",
    "if nmerge.duplicated().any():\n",
    "    print('True.Duplicates, drop\\n')\n",
    "    nmerge.drop_duplicates(inplace=True)\n",
    "else:\n",
    "    print('False. No Duplicates, no drop\\n')\n",
    "print(f'######\\n\\nNulls:\\n{nmerge.isna().sum()}\\n')\n",
    "print(f'######\\nnew shape:{nmerge.shape}')\n",
    "if nmerge.columns[0]=='key_0':\n",
    "    print('Newly created key columns not dropped\\nColumns now dropped')\n",
    "    nmerge.drop(columns=nmerge.columns[[0,1,2]],inplace=True)\n",
    "else:\n",
    "    print('Columns not dropped')\n",
    "nmerge.head(3)\n",
    "# change directory for exporting of df\n",
    "os.chdir('J:\\\\Brainstation\\\\BS Git\\\\human Imact\\\\Nutrient_loading')\n",
    "ksub.to_csv('kjeldahl.csv')\n",
    "nsub.to_csv('nitrate.csv')\n",
    "asub.to_csv('ammonia.csv')\n",
    "nmerge.to_csv('nit_merged_mean.csv')\n",
    " [markdown]\n",
    "# Due to issues with formatting the datetime index, I haven't been able to get the frequency to set properly and, consequently, forecasting likely will be unhelpful. Results and conclusions drawn from the model should be understood with this in mind as it's difficult to assess the accuracy and performance of the model, particularly as a forecasting model. There are inferences that can be drawn about the relationships captured by the model between the 3 features.\n",
    "# \n",
    "# The model had the most difficulty handling Nitrate values. P-values were less than 0.05 only in 4 lag iterations of the model - spiking beyond 0.45 in the next tier of 8 lags. The results do show, however, in the 4 and subsequent 8 lag iterations, nitrates has strong autocorrelation at the 4 week intervals (L1,L4,L8) which may provide insight into its sources. The 4 week lag period could be related to the transport time of nitrates through the watershed, which begins far inland before draining into the river systems and ultimately the bay. Monthly cyclicality of anthrogenic comunity services - such as agricultral practices, sewage treatment, would correlate  \n",
    "# \n",
    "# There is some noted influence of ammonia on Kjeldahl values at lag 1 and lag 4, which could be explained by the conversion of organic nitrates into ammonia in the mineralisation process. The Kjelahl method also is measured as ammonia measured while processing water samples with a catalyst agent. Indirect influences of biological processes and existing levels of ammonia could explain this relationship; however, this is only one observation and not the primary driver of Kjeldahl concentrations or vice-versa.\n",
    "# \n",
    "# Kjeldahl is more complex with of autocorrelation at lag 1,2,8,9, influences on ammonia at lag 1,2,4 and nitrogen  while also impacting ammonia at week 1,2,4, and nitro at possible 1 and 2; however, this does lend confidence to its use as a proxy as the organic form of nitrogen in the ecosystem. Decay and decomposition would impact ammonia and nitrate concentrations.\n",
    "# \n",
    "# Low values of AIC and BIC are encouraging that the model performed reasonably well in explaining the existing data; however, no conclusions can be made about the model's predictive power without being rerun with corrected datasets and evaluation of its performance through error metrics.\n",
    "# \n",
    "# I am confident though in proceeding with the base assumptions necessary to proceed its application in the development of the API (index).\n",
    "fig, ax = plt.subplots(figsize=(8,15))\n",
    "ax1=ax.twinx()\n",
    "plt.subplot(3,1,1)\n",
    "nmerge.ResultValue_K.hist(color='g',label='Kjeldahl')\n",
    "plt.subplot(3,1,2)\n",
    "nmerge.ResultValue_A.hist(color='y',label='Ammonia')\n",
    "plt.subplot(3,1,3)\n",
    "nmerge.ResultValue_N.hist(color='r',label='Nitrate')\n",
    "#plt.subplot(3,1,4)\n",
    "plt.legend()\n",
    "plt.suptitle('Distribution of Nitrogen Compounds Concentration Measurements')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# fig,ax = plt.subplots(figsize=(8,8))\n",
    "sns.lineplot(x=nmerge.index, y=nmerge.ResultValue_K, label='[Kjeldahl]',color='green',ax=ax)\n",
    "#plt.xlabel*('Concentrations of Organic Nitrogen (Kjel)')\n",
    "ax1=ax.twinx()\n",
    "sns.lineplot(x=nmerge.index, y=nmerge.ResultValue_A, label='[Ammonia]',color='yellow',ax=ax1)\n",
    "sns.lineplot(x=nmerge.index, y=nmerge.ResultValue_N, label='[Nitrates]',color='red',ax=ax1)\n",
    "plt.title('Concentrations of Nitrogen compounds in Tampa Bay, 1990-2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Concentrations of Compounds (mg/l)')\n",
    "plt.yscale('log')\n",
    "nmerge['K_moving_average'] = nmerge.ResultValue_K.rolling(window=36).mean()\n",
    "nmerge['A_moving_average'] = nmerge.ResultValue_A.rolling(window=36).mean()\n",
    "nmerge['N_moving_average'] = nmerge.ResultValue_N.rolling(window=36).mean()\n",
    "plt.figure(figsize=(7,7))\n",
    "nmerge.ResultValue_K.plot(color='b')\n",
    "nmerge.K_moving_average.plot(color='g',lw=4)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.figure(figsize=(7,7))\n",
    "nmerge.A_moving_average.plot(color='y',lw=4)\n",
    "nmerge.N_moving_average.plot(color='r',lw=4)\n",
    "plt.yscale('log')\n",
    "plt.title('Log Scale Moving Average of Ammonia and Nitrate Nitrogen-compounds in Tampa Bay, 1990-2024')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cc216-fdb9-4c44-81fd-b2d1fccca1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "[markdown]\n",
    "# Measurements of Nitrogen in Kjeldahl can be understood as measuerments of organic nitrogen found water systems.\n",
    "# Measurements of Ammonia (NH3/NH4+) can be understood as anthropogenic sources of nitrogen\n",
    "# Measurements of Nitrates/Nitrites are anthropogenic sources of nitrogen.\n",
    "# \n",
    "# In order to use Ammonia and Nitrates as factors for the API while using Kjeldahl measurements as a feature of the marine ecosystem, it has to be shown that there's correlation between NH3,NO3 ~ Kjel.<br>\n",
    "# \n",
    "# This study uses vector autoregression (VAR) to look at these relationships in a time series.\n",
    "# \n",
    "# Sampling frequency, measurements have been standardized for frequency and units.<br>\n",
    "# Missing values have been interpolated. \n",
    "### nmerge df Checkpoint\n",
    "#$nmerge_backup=nmerge.copy()\n",
    "#nmerge=nmerge_back.copy()\n",
    "nmerge.index\n",
    "nmergeK=pd.Series(nmerge.ResultValue_K)\n",
    "nmergeK.index=nmerge.index\n",
    "# Assuming your time series data is in a pandas Series called 'series'\n",
    "# and has a datetime index\n",
    "# Perform seasonal decomposition\n",
    "result = seasonal_decompose(pd.Series(nmergeK), model='additive')  # or model='multiplicative'\n",
    "# Access the decomposed components\n",
    "trend = result.trend\n",
    "seasonal = result.seasonal\n",
    "residual = result.resid\n",
    "# Plot the decomposed components\n",
    "result.plot()\n",
    "# Access individual components as pandas Series\n",
    "print(trend)\n",
    "print(seasonal)\n",
    "print(residual)\n",
    "nmerge.iloc[:,:-3]\n",
    "nmerge.iloc[:,-3:].describe()\n",
    "VAR_data=nmerge.iloc[:,0:3]\n",
    "rolling_avg_data=nmerge.iloc[:,:-3]\n",
    "scaler1=MinMaxScaler()\n",
    "scaler2=MinMaxScaler()\n",
    "scaler1.fit(VAR_data)\n",
    "VAR_scaled = pd.DataFrame(scaler1.transform(VAR_data), columns=VAR_data.columns, index=VAR_data.index)\n",
    "scaler2.fit(rolling_avg_data)\n",
    "rolling_scaled=pd.DataFrame(scaler2.transform(rolling_avg_data), columns=rolling_avg_data.columns, index=rolling_avg_data.index)\n",
    "plt.subplots(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(VAR_scaled,lw=4)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(rolling_scaled,lw=4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Data has already been scaled with MinMax\n",
    "# Splits scaled data\n",
    "train_data, test_data = train_test_split(VAR_data, test_size=0.3, shuffle=False)\n",
    "# Scale data\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data_scaled = pd.DataFrame(scaler.transform(train_data), columns=train_data.columns, index=train_data.index)\n",
    "test_data_scaled = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns, index=test_data.index) \n",
    "VAR_data.head(2)\n",
    "train_data_scaled\n",
    "test_data_scaled\n",
    " [markdown]\n",
    "# ## Development 99.206KM COASTLINE NOT INCLUDING WATERWAYS\n",
    " [markdown]\n",
    "# Here is the introduction of construction, real estate and development influences on the bay.\n",
    "# It is difficult to retrieve consisten data for land usage, development and coastal construction, due to privacy rights, hidden data and more.\n",
    "# \n",
    "# Possible methods:\n",
    "# - Aggregation of construction data through issued permits and property records\n",
    "# - Land use categories\n",
    "# - Construction. Looking at permits, construction dates and even sales can inform development rates.\n",
    "# \n",
    "# Initially, this study proposed looking at waterfront properties; the full property record dataset was used for 2 reasons. 1- Quantity of Data. Annual figures for construction would have been too low. More importantly, because of the size of the watershed (extending beyond county lines),  fertilizers, sewage treatment and other anth.gen. substances can be traced far inland. These compounds have been shown to be the most damaging to the Tampa Bay ecosystem. Input from increased activity, development and subsequent contamination of the water that feeds the bay should be evaluated by development at greater resolution. These have an equal weighted influence on anthropogenic pressures on the local marine ecosystem.\n",
    "# \n",
    "# Data:\n",
    "# A CSV has consolidated all of the property records. These include construction dates, which are used as proxies for development rates.\n",
    "# The primary features of interest are\n",
    "# - Property Acreage\n",
    "# - Year Built\n",
    "# - Number of structures built\n",
    "# - Land type\n",
    "# - Total square footage of structure\n",
    " [markdown]\n",
    "# ![AB_SOUTH_2.png](attachment:4e9920ac-f6d1-42e6-b583-b94bbd623f58.png)\n",
    "proprx\n",
    " [markdown]\n",
    "# #### Processing df from initial dev of models (CODE OUTDATED)\n",
    "AB_2.info()\n",
    "dflist=[\n",
    "AB_1,\n",
    "AB_2,\n",
    "AB_3,\n",
    "AB_4,\n",
    "AB_5,\n",
    "AB_6,\n",
    "AB_7,\n",
    "GIB_8,\n",
    "RIV_1]\n",
    "dfprop=pd.DataFrame(data=AB_2)\n",
    "# Combine all sections of property data\n",
    "# dfprop = 'Dataframe: Property'\n",
    "for x in dflist:\n",
    "    print(x.shape)\n",
    "    dfprop=pd.concat([dfprop,x],axis=0)\n",
    "dfprop1=dfprop.copy()\n",
    "dfprop.head(5)\n",
    "dfprop.shape\n",
    "AB_1.head(1)\n",
    "AB_2.head(1)\n",
    "AB_3.head(3)\n",
    "dfprop.info()\n",
    "\"\"\"\n",
    "Columns for Selection Pressure analysis\n",
    "3,\n",
    "8,\n",
    "14,\n",
    "15,\n",
    "25,\n",
    "26,\n",
    "30,\n",
    "36,\n",
    "40,\n",
    "42\n",
    "\"\"\"\n",
    "#Selecting columns of interest\n",
    "dfprop.drop_duplicates(inplace=True)\n",
    "dfprop=dfprop[dfprop.columns[[0,3,13,14,15,25,27,30,36,40,42]]]\n",
    "RIV_1=RIV_1[RIV_1.columns[[0,3,13,14,15,25,27,30,36,40,42]]]\n",
    "RIV_1.shape\n",
    "dfprop.shape\n",
    "dfprop1\n",
    "# Join remaining df csv for a consolidated table\n",
    "dfprop=pd.concat([dfprop,RIV_1])\n",
    "#null check\n",
    "dfprop.isna().sum()\n",
    "# To be consildated into higher aggregate categories: Residential, Commercial, Industrial, Other\n",
    "dfprop.PropertyType.value_counts()\n",
    "# Slight cleaning of the data\n",
    "# Some of the grids overlapped so these rows must be accounted for\n",
    "dfprop.duplicated().value_counts()\n",
    "dfprop.info()\n",
    "dfprop.isna().sum()\n",
    "dfprop.Acreage\n",
    "dfprop.PropertyType.value_counts()\n",
    " [markdown]\n",
    "# #### Data Cleaning\n",
    "dfprop.PropertyType.isna().sum()\n",
    "dfprop.PropertyType.fillna('undefined',inplace=True)\n",
    "dfprop.duplicated().sum()\n",
    "# Get rid of redundant data rows|\n",
    "dfprop.drop_duplicates(inplace=True)\n",
    "dfprop.isna().sum()\n",
    "dfprop.duplicated().value_counts()\n",
    "dfprop[dfprop.SiteCity.isna()==True]\n",
    "dfprop[dfprop.VacantImproved.isna() & dfprop.YearBuilt>0]\n",
    "# not all columns are relevant and their NaN, null or 0 status is not important\n",
    "dfprop.isna().sum()\n",
    "# Improved plots that have been misclassified\n",
    "dfprop[dfprop.VacantImproved.isna() & dfprop.YearBuilt>0].VacantImproved.fillna('Improved',inplace=True)\n",
    "# Looking for common traits - perhaps it speaks to why these properties were misclassified and and relevance\n",
    "dfprop[dfprop.YearBuilt==0].PropertyType.unique()\n",
    "dfprop\n",
    "dfprop.sort_values(by='YearBuilt').YearBuilt.value_counts().sort_index()\n",
    " [markdown]\n",
    "# #### Full Property Dataset\n",
    "# Data Description file also found in repo\n",
    "# df check\n",
    "proprx.head(2)\n",
    "#col check\n",
    "# ACT ='Actual Year Built'\n",
    "proprx.info()\n",
    "# No properties missing built date\n",
    "proprx.isna().sum()\n",
    "#lookat act values\n",
    "#there are 0 values\n",
    "proprx.ACT.head(20)\n",
    "print(proprx.ACT.value_counts())\n",
    "print('\\nSHAPE: ',proprx.ACT.shape)\n",
    "proprx[proprx.ACT==0].shape\n",
    " [markdown]\n",
    "# About 10% loss of records, which is acceptable given the number of records.\n",
    "proprx\n",
    "#isolate features\n",
    "proprx2=proprx.groupby(['SITE_CITY','ACT','ACREAGE','tUNITS','tBLDGS','HEAT_AR'])['ACT'].agg({'mean','min','max'}).reset_index()\n",
    "# Agg House construction by year\n",
    "# Year is the most granular offered by dataset\n",
    "annbuilt=proprx2.groupby('ACT').ACT.sum()\n",
    "annbuilt=annbuilt.to_frame().reset_index(names=['yrbuilt'])\n",
    "# isolate to 2024\n",
    "annbuilt=annbuilt[annbuilt.yrbuilt.between(1989,2024)]\n",
    "annbuilt.head(2)\n",
    " [markdown]\n",
    "# Primary features of interest for simple model are Year Built, Acreage, totalGrossAreaSqft.\n",
    "# \n",
    "# Understanding the relationship between development per property and population density and growth will be a calculated feature used in the construction of the HII-aquatic index.\n",
    "annbuilt=annbuilt.groupby('yrbuilt').ACT.agg({'sum','mean','min','max'})\n",
    "annbuilt.reset_index(inplace=True)\n",
    "annbuilt.head(2)\n",
    "annbuilt=annbuilt[['yrbuilt','sum']]\n",
    "annbuilt.head(2)\n",
    "# Look at construction over time for all of HC\n",
    "plt.figure(figsize=(6,6))\n",
    "annbuilt.plot('yrbuilt','sum')\n",
    "plt.yscale('log')\n",
    "plt.title('construction over time for all of HC')\n",
    "plt.show()\n",
    " [markdown]\n",
    "# Construction Data needs to be scaled so that it can be used in the index\n",
    "annbuilt.index=annbuilt.yrbuilt\n",
    "annbuilt.drop(columns='yrbuilt',inplace=True)\n",
    "# Creating values scaled to a range of 0-1\n",
    "scaler = MinMaxScaler()\n",
    "#fit,transform\n",
    "houses_built_scaled = scaler.fit_transform(annbuilt.values.reshape(-1, 1))\n",
    "# New standardized df\n",
    "houses_built_scaled\n",
    "construction_scale=pd.DataFrame(houses_built_scaled)\n",
    "construction_scale['year']=annbuilt.index\n",
    "construction_scale.rename(columns={0:'scaled'},inplace=True)\n",
    " [markdown]\n",
    "# ### Housing, Population Density and Change\n",
    " [markdown]\n",
    "# The impact of coastal development extends beyond the shoreline. In an estuarine environment, the watersheds that feed into the estuary (or in the case the bay) are equally important. Using a highway as a physical boundary overlapped with a map of the various watersheds, a representative set of properties, the type, their acreage and the year that they were built will be used as an estimate and proxy for coastline development.  Based on research literature, concentration of non-organic nitrates and phosphates will serve as a proxy for the introduction of artificial damaging compounds - however, in the construction of more complex models, more granular data can be used.\n",
    "## New Construction estimates by year\n",
    "# Get annual change of homes built on hillsborough bay coast from 1990-present\n",
    "perchange=[]\n",
    "year=1989\n",
    "year1=[]\n",
    "x_base=[]\n",
    "x1=[]\n",
    "for x in range(1989,1989+len(dfyearbuilt)):\n",
    "    if x==0:\n",
    "        continue\n",
    "    elif x==1988+len(dfyearbuilt):\n",
    "        break\n",
    "    else:\n",
    "        perchange.append(round((dfyearbuilt[x+1]/dfyearbuilt[x])*100,3))\n",
    "        x_base.append(dfyearbuilt[x])\n",
    "        x1.append(dfyearbuilt[x+1])\n",
    "    year+=1\n",
    "    year1.append(year)\n",
    "perchange[0:5]\n",
    "dfperchange=pd.DataFrame()\n",
    "dfperchange['percentchange']=perchange\n",
    "dfperchange['x']=x_base\n",
    "dfperchange['x+1']=x1\n",
    "dfperchange['year']=year1\n",
    "dfperchange.head(5)\n",
    "dfprop[dfprop.YearBuilt>1989].groupby(['YearBuilt']).TotalNumBuildings.agg(['min', 'max', 'std', 'sum'])\n",
    "dfacreage\n",
    "# Get annual change of Total buildings built on hillsborough bay coast from 1990-present\n",
    "perchange_build=[]\n",
    "year_build=1989\n",
    "year1_build=[]\n",
    "x_base1=[]\n",
    "x2=[]\n",
    "for x in range(1989,1989+len(dftotalunit)):\n",
    "    if x==0:\n",
    "        continue\n",
    "    elif x==1988+len(dfyearbuilt):\n",
    "        break\n",
    "    else:\n",
    "        perchange_build.append(round((dftotalunit[x+1]/dftotalunit[x])*100,3))\n",
    "        x_base1.append(dftotalunit[x])\n",
    "        x2.append(dftotalunit[x+1])\n",
    "    year_build+=1\n",
    "    year1_build.append(year_build)\n",
    "# Describes number of houses built in subset data, total number of units per built structure\n",
    "# Multi-unit buildings have an increased environmental impact per individual parcel\n",
    "dfperchange_unit\n",
    " [markdown]\n",
    "# #### Population\n",
    "### Looking into population statistics nolw\n",
    "dfpop=pd.DataFrame()\n",
    "dfpop['year']=0\n",
    "dfpop['sumpop']=0\n",
    "dfpop_staged.iloc[:,1]\n",
    "dfpop_staged\n",
    "## This file was originally a geodatabase file that has been read in as a CSV to access the non-geospatial data.\n",
    "dfpop_staged\n",
    "dfpop_staged.info()\n",
    "est_pop=dfpop_staged.iloc[0:,21:29]\n",
    "#area=sum(dfpop_staged.iloc[1:,10])\n",
    "# Set up series for 5 year intervals\n",
    "f=pd.Series()\n",
    "counter=1990\n",
    "for x in nont.columns:\n",
    "    f[counter]=sum(est_pop[x])\n",
    "    counter+=5\n",
    "f=f.to_frame().reset_index()\n",
    "#calculate population density\n",
    "population_density=f[0]/area\n",
    "year=[1990,1995,2000,2005,2010,2015,2020,2025]\n",
    "model=LinearRegression()\n",
    "X_train,X_test,y_train,y_test = train_test_split(popden.year,popden.density, test_size=0.3, shuffle=False)\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test) \n",
    "den.to_csv('population_density.csv')\n",
    "os.chdir('J:\\\\Brainstation\\\\Capstone Git\\\\data\\\\human Imact\\\\2024 Hillsborough Shape Files')\n",
    "dev=gpd.read_file('Tampa_Bay_Development.shp')\n",
    "dev_df=pd.read_csv('hc_shp.csv',encoding='cp1252',low_memory=False)\n",
    "dev_df.info()\n",
    "dev_df.head(10)\n",
    "pwd\n",
    "plt.figure(figsize=(50,50))\n",
    "dev.plot(column='FLUCSDESC',figsize=(50,50))\n",
    "#plt.plot(dev)\n",
    "#plt.legend()\n",
    "#plt.savefig('test.png')\n",
    "plt.show()\n",
    "os.chdir('J:\\\\Brainstation\\\\Capstone Git\\\\data\\\\human Imact\\\\Population Data')\n",
    "pop_metro=pd.read_csv('tampa_metro_population.csv',header=0)\n",
    "pop_metro.info()\n",
    "pop_metro\n",
    "sns.lineplot(x='Year',y='Population',data=pop_metro)\n",
    "plt.xlabel='year'\n",
    "plt.ylabel='Population'\n",
    "sns.scatterplot(x='Year',y=np.log(pop_metro.Population),data=pop_metro)\n",
    "plt.set_xlabel='Population (per 100k)'\n",
    "plt.title('Tampa Regional Population Over Time')\n",
    "plt.show()\n",
    "LinRegLog=LinearRegression()\n",
    "X=pop_metro.Year.to_frame()\n",
    "y=pop_metro.Population\n",
    "LinRegLog.fit(X,y)\n",
    "LinRegLog.score(X,y)\n",
    "LinRegLog.coef_\n",
    "LinRegLog.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "LinRegLog.fit(X_train,y_train)\n",
    "LinRegLog.score(X_train,y_train)\n",
    "LinRegLog.score(X_test,y_test)\n",
    "pwd\n",
    "os.chdir('J:\\\\Brainstation\\\\BS Git\\\\Property Records\\\\GIS\\\\Zips\\\\')\n",
    "os.chdir('J:\\\\Brainstation\\\\Capstone Git\\\\data\\\\human Imact\\\\Population Data')\n",
    "pop_metro=pd.read_csv('tampa_metro_population.csv',header=0)\n",
    "pop_metro.info()\n",
    "pop_metro\n",
    "sns.lineplot(x='Year',y='Population',data=pop_metro)\n",
    "plt.xlabel='year'\n",
    "plt.ylabel='Population'\n",
    "sns.scatterplot(x='Year',y=np.log(pop_metro.Population),data=pop_metro)\n",
    "plt.set_xlabel='Population (per 100k)'\n",
    "plt.title('Tampa Regional Population Over Time')\n",
    "plt.show()\n",
    "LinRegLog=LinearRegression()\n",
    "X=pop_metro.Year.to_frame()\n",
    "y=pop_metro.Population\n",
    "LinRegLog.fit(X,y)\n",
    "LinRegLog.score(X,y)\n",
    "LinRegLog.coef_\n",
    "LinRegLog.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "LinRegLog.fit(X_train,y_train)\n",
    "LinRegLog.score(X_train,y_train)\n",
    "LinRegLog.score(X_test,y_test)\n",
    "pwd\n",
    "os.chdir('J:\\\\Brainstation\\\\BS Git\\\\Property Records\\\\GIS\\\\Zips\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs",
   "language": "python",
   "name": "bs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
